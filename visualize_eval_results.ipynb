{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import helper_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable to the path of the csv file containing the results.\n",
    "filepath = (\n",
    "    '/path/to/results.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results.\n",
    "data = []\n",
    "with open(filepath, 'r') as fread:\n",
    "    reader = csv.DictReader(fread)\n",
    "    for row in reader:\n",
    "        row['sed'] = int(row['sed'])\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: class\n",
      "Success Rate: 0.40280433397068194\n",
      "Non-zero mean: 3.297758804695838\n",
      "Non-zero std: 2.1541266875484344\n",
      "\n",
      "\n",
      "Rule: ooe\n",
      "Success Rate: 0.03357531760435572\n",
      "Non-zero mean: 9.21643192488263\n",
      "Non-zero std: 5.365916820560861\n",
      "\n",
      "\n",
      "Rule: affordance\n",
      "Success Rate: 0.4435946462715105\n",
      "Non-zero mean: 3.2245131729667813\n",
      "Non-zero std: 2.1011084779967604\n",
      "\n",
      "\n",
      "Rule: utility\n",
      "Success Rate: 0.6124920331421287\n",
      "Non-zero mean: 2.932565789473684\n",
      "Non-zero std: 2.193829263588442\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to print aggregate eval metrics per organizational schema. \n",
    "for rule in [\"class\", \"ooe\", \"affordance\", \"utility\"]:\n",
    "    data_filtered = [d for d in data if d[\"rule\"] == rule]\n",
    "    sr, nz_mean, nz_std = helper_eval.calculate_evaluation_metrics(data_filtered)\n",
    "\n",
    "    print(\"Organizational schema: {}\".format(rule))\n",
    "    print(\"Success Rate: {}\".format(sr))\n",
    "    print(\"Non-zero mean: {}\".format(nz_mean))\n",
    "    print(\"Non-zero std: {}\".format(nz_std))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rgmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
